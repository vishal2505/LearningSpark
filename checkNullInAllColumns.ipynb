{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "spylon-kernel",
   "display_name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Checking Null in All the columns in a csv file\n",
    "In this example, we'll see how to check for null values in all of the available columns using foldLeft ().\n",
    "We can use column.isNull function one by one for all the columns but if dataframe has 100s of columns, then this method is not appropriate. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, IntegerType, TimestampType}\n",
       "inputSchema: org.apache.spark.sql.types.StructType = StructType(StructField(Date,StringType,false), StructField(Transaction,IntegerType,false), StructField(Name,StringType,false), StructField(Transaction_ID,StringType,false))\n"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, IntegerType, TimestampType}\n",
    "\n",
    "val inputSchema = StructType(List(\n",
    "    StructField(\"Date\", StringType, false),\n",
    "    StructField(\"Transaction\", IntegerType, false),\n",
    "    StructField(\"Name\", StringType, false),\n",
    "    StructField(\"Transaction_ID\", StringType, false)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----------+----+--------------+\n",
      "|      Date|Transaction|Name|Transaction_ID|\n",
      "+----------+-----------+----+--------------+\n",
      "|29-10-2020|       null| ABC|          tid1|\n",
      "|30-10-2020|        120| ABC|          tid2|\n",
      "|      null|        121| ABC|          tid3|\n",
      "+----------+-----------+----+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "inputDataDF: org.apache.spark.sql.DataFrame = [Date: string, Transaction: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "val inputDataDF = spark.read\n",
    "        .option(\"header\",true)\n",
    "        .schema(inputSchema)\n",
    "        .csv(\"./data/tran_data.csv\")\n",
    "inputDataDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Transaction: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Transaction_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDataDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ids: org.apache.spark.sql.DataFrame = [Transaction_ID: string, NullColumns: string]\n"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "var ids = spark.emptyDataFrame\n",
    "    .withColumn(\"Transaction_ID\",lit(\"\"))\n",
    "    .withColumn(\"NullColumns\",lit(\"\"))"
   ]
  },
  {
   "source": [
    "### Approach - 1: Chekcing with individual columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+\n",
      "|Transaction_Id|\n",
      "+--------------+\n",
      "|          tid1|\n",
      "|          tid3|\n",
      "|          tid5|\n",
      "|          tid6|\n",
      "|          tid7|\n",
      "|          tid8|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDataDF.select(\"Transaction_Id\").where(col(\"Date\").isNull || col(\"Transaction\").isNull || col(\"Name\").isNull).show()"
   ]
  },
  {
   "source": [
    "### Approach - 2: Using foldLeft()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+-----------+\n",
      "|Transaction_ID|NullColumns|\n",
      "+--------------+-----------+\n",
      "|tid3          |Date       |\n",
      "|tid6          |Date       |\n",
      "|tid8          |Date       |\n",
      "|tid1          |Transaction|\n",
      "|tid5          |Transaction|\n",
      "|tid6          |Transaction|\n",
      "|tid7          |Transaction|\n",
      "|tid8          |Name       |\n",
      "|tid3          |Date       |\n",
      "|tid6          |Date       |\n",
      "|tid8          |Date       |\n",
      "|tid1          |Transaction|\n",
      "|tid5          |Transaction|\n",
      "|tid6          |Transaction|\n",
      "|tid7          |Transaction|\n",
      "|tid8          |Name       |\n",
      "|tid3          |Date       |\n",
      "|tid6          |Date       |\n",
      "|tid8          |Date       |\n",
      "|tid1          |Transaction|\n",
      "+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDataDF.columns.foldLeft(inputDataDF) { (inputDataDF,colName) => \n",
    "    ids = ids.unionByName(inputDataDF.select(\"Transaction_Id\").where(col(colName).isNull)\n",
    "        .withColumn(\"NullColumns\",lit(colName)))\n",
    "    inputDataDF\n",
    "}\n",
    "ids.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+-------------------+\n",
      "|Transaction_ID|NullColumns        |\n",
      "+--------------+-------------------+\n",
      "|tid8          |[Date, Name]       |\n",
      "|tid5          |[Transaction]      |\n",
      "|tid7          |[Transaction]      |\n",
      "|tid1          |[Transaction]      |\n",
      "|tid6          |[Transaction, Date]|\n",
      "|tid3          |[Date]             |\n",
      "+--------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "finalDF: org.apache.spark.sql.DataFrame = [Transaction_ID: string, NullColumns: array<string>]\n"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "val finalDF = ids.dropDuplicates(\"Transaction_ID\",\"NullColumns\")\n",
    "    .groupBy(\"Transaction_ID\")\n",
    "    .agg(collect_list(col(\"NullColumns\")).as(\"NullColumns\"))\n",
    "\n",
    "finalDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}