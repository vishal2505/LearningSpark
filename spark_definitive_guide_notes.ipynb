{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "spylon-kernel",
   "display_name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spark - The Definitive Guide (DataBricks)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"images/spark_history.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "myRange: org.apache.spark.sql.DataFrame = [number: bigint]\n"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "val myRange = spark.range(1000).toDF(\"number\")\n",
    "myRange.show(10)"
   ]
  },
  {
   "source": [
    "## Dataframe Transformation\n",
    "\n",
    "Tranformations are lazy. There were only executed when we call an Action on that.\n",
    "\n",
    "### Lazy Evaluation\n",
    "An example of this might be “predicate pushdown”. If we build a large Spark job consisting of narrow dependencies, but specify a filter at the end that only requires us to fetch one row from our source data, the most efficient way to execute this is to access the single record that we need. Spark will actually optimize this for us by pushing the filter down automatically."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "divBy2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [number: bigint]\n"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "val divBy2 = myRange.where(\"number % 2 == 0\")\n",
    "divBy2.show(10)"
   ]
  },
  {
   "source": [
    "### Read flight data csv file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "flightData2015: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "val flightData2015 = spark.read\n",
    "                .option(\"inferSchema\",true)\n",
    "                .option(\"header\", true)\n",
    "                .csv(\"./data/2015-summary.csv\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "== Physical Plan ==\n*(1) Sort [count#50 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#50 ASC NULLS FIRST, 200), true, [id=#82]\n   +- FileScan csv [DEST_COUNTRY_NAME#48,ORIGIN_COUNTRY_NAME#49,count#50] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/vishalmishra/projects/github_repos/LearningSpark/data/2015-summary...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n\n\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "res6: Array[org.apache.spark.sql.Row] = Array([United States,Singapore,1], [Moldova,United States,1])\n"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "source": [
    "### Exercise - 1:\n",
    "We will use the max function, to find out what is the maximum number of flights to and from any given location are ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SQL way\n",
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n",
      "DataFrame way\n",
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.max\n"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "//SQL way\n",
    "println(\"SQL way\")\n",
    "spark.sql(\"select max(count) from flight_data_2015\").show()\n",
    "\n",
    "//DataFrame way\n",
    "println(\"DataFrame way\")\n",
    "import org.apache.spark.sql.functions.max\n",
    "flightData2015.select(max(\"count\")).show()\n"
   ]
  },
  {
   "source": [
    "### Exercise - 2:\n",
    "Find out the top five destination countries in the data set?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SQL way\n",
      "[United States,411352]\n",
      "[Canada,8399]\n",
      "[Mexico,7140]\n",
      "[United Kingdom,2025]\n",
      "[Japan,1548]\n",
      "\n",
      "DataFrame way\n",
      "[United States,411352]\n",
      "[Canada,8399]\n",
      "[Mexico,7140]\n",
      "[United Kingdom,2025]\n",
      "[Japan,1548]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "top5Dest: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, sum(count): bigint]\n",
       "top5DestDF: Unit = ()\n"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "println(\"SQL way\")\n",
    "val top5Dest = spark.sql(\"\"\"\n",
    "    select DEST_COUNTRY_NAME, sum(count) from flight_data_2015\n",
    "    group by DEST_COUNTRY_NAME\n",
    "    order by sum(count) DESC\n",
    "    Limit 5\n",
    "    \"\"\")\n",
    "\n",
    "top5Dest.collect.foreach(println)\n",
    "\n",
    "println(\"\\nDataFrame way\")\n",
    "val top5DestDF = flightData2015\n",
    "            .groupBy(\"DEST_COUNTRY_NAME\")\n",
    "            .sum(\"count\")\n",
    "            .withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    "            .sort(desc(\"destination_total\"))\n",
    "            .limit(5)\n",
    "            .collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "== Physical Plan ==\nTakeOrderedAndProject(limit=5, orderBy=[destination_total#209L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#48,destination_total#209L])\n+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#48], functions=[sum(cast(count#50 as bigint))])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#48, 200), true, [id=#379]\n      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#48], functions=[partial_sum(cast(count#50 as bigint))])\n         +- FileScan csv [DEST_COUNTRY_NAME#48,count#50] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/vishalmishra/projects/github_repos/LearningSpark/data/2015-summary...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "top5DestDF: Unit = ()\n"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "val top5DestDF = flightData2015\n",
    "            .groupBy(\"DEST_COUNTRY_NAME\")\n",
    "            .sum(\"count\")\n",
    "            .withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    "            .sort(desc(\"destination_total\"))\n",
    "            .limit(5)\n",
    "            .explain()"
   ]
  },
  {
   "source": [
    "## Datasets\n",
    "\n",
    "DataFrames are a distributed collection of objects of type Row but Spark also allows JVM users to create\n",
    "their own objects (via case classes or java beans) and manipulate them using function programming concepts. For instance, rather than creating a range and manipulating it via SQL or DataFrames, we can manipulate it just as we might manipulate a local scala collection. We can map over the values with a user defined function, and convert it into a new arbitrary case class object. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In essence, within the Structured APIs, there are two more APIs, the “untyped” DataFrames and the “typed” Datasets. To say that DataFrames are untyped is a bit of a misnomer, they have types but Spark maintains them completely and only checks whether those types line up to those specified in the schema at runtime. Datasets, on the other hand, check whether or not types conform to the specification at compile time. Datasets are only available to JVM based languages (Scala and Java) and we specify types with case classes or Java beans."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Columns\n",
    "Columns can represent a simple type like an integer or string, a complex types like an array or map, or a null value. Spark tracks all of this type information to you and has a variety of ways that you can transform columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Rows\n",
    "\n",
    "There are two ways of getting data into Spark, through Rows and Encoders. \n",
    "\n",
    "Row objects are the most general way of getting data into, and out of, Spark and are available in all languages. \n",
    "\n",
    "Each record in a DataFrame must be of Row type.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Overview of Structured API Execution\n",
    "\n",
    "1. Write DataFrame/Dataset/SQL Code\n",
    "2. If valid code, Spark converts this to a Logical Plan\n",
    "3. Spark transforms this Logical Plan to a Physical Plan\n",
    "4. Spark then executes this Physical Plan on the cluster\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"images/df_execution_plan.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logical Planning\n",
    "The first phase of execution is meant to take user code and convert it into a logical plan.\n",
    "This logical plan only represents a set of abstract transformations that do not refer to executors or drivers, it’s purely to convert the user’s set of expressions into the most optimized version. It does this by converting user code into an unresolved logical plan. This unresolved because while your code may be valid, the tables or columns that it refers to may or may not exist. \n",
    "\n",
    "Spark uses the catalog, a repository of all table and DataFrame information, in order to resolve\n",
    "columns and tables in the analyzer. \n",
    "\n",
    "The analyzer may reject the unresolved logical plan if it the required table or\n",
    "column name does not exist in the catalog. If it can resolve it, this result is passed through the optimizer, a collection of rules, which attempts to optimize the logical plan by pushing down predicates or selections."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"images/logical_planning.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Physical Planning\n",
    "\n",
    "After successfully creating an optimized logical plan, Spark then begins the physical planning process. \n",
    "\n",
    "The physical plan, often called a Spark plan, specifies how the logical plan will execute on the cluster by generating different physical execution strategies and comparing them through a cost model. \n",
    "\n",
    "An example of the cost comparison might be choosing how to perform a given join by looking at the physical attributes of a given table (how big the table is or how big its partitions are.)\n",
    "\n",
    "Physical planning results in a series of RDDs and transformations. This result is why you may have heard Spark referred to as a compiler, it takes queries in DataFrames, Datasets, and SQL and compiles them into RDD transformations for you."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"images/physical_planning.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Schemas\n",
    "\n",
    "A schema defines the column names and types of a DataFrame. \n",
    "\n",
    "Deciding whether or not you need to define a schema prior to reading in your data depends your use case. Often times for ad hoc analysis, schema on read works just fine (although at times it can be a bit slow with plain text file formats like csv or json). However, this can also lead to precision issues like a long type incorrectly set as an integer when reading in a file. \n",
    "\n",
    "When using Spark for production ETL, it is often a good idea to define your schemas\n",
    "manually, especially when working with untyped data sources like csv and json because schema inference can vary depending on the type of data that you read in.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "//Auto Loading of schema\n",
    "\n",
    "val df=spark.read.format(\"json\")\n",
    "    .load(\"data/2015-summary.json\")\n",
    "    .schema"
   ]
  },
  {
   "source": [
    "A schema is a StructType made up of a number of fields, StructFields, that have a name, type, and a boolean flag which specifies whether or not that column can contain missing or null values. \n",
    "Schemas can also contain other StructType (Spark’s complex types)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "mannualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(COUNT,LongType,false))\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "//Mannually defning the schema\n",
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "\n",
    "val mannualSchema = new StructType(Array(\n",
    "        new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "        new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "        new StructField(\"COUNT\", LongType, false)))\n",
    "\n",
    "val df=spark.read.format(\"json\")\n",
    "    .schema(mannualSchema)\n",
    "    .load(\"data/2015-summary.json\")"
   ]
  },
  {
   "source": [
    "### Columns\n",
    "There are a lot of different ways to construct and or refer to columns but the two simplest ways are with the col or column functions. To use either of these functions, we pass in a column name.\n",
    "\n",
    "%scala\n",
    "\n",
    "import org.apache.spark.sql.functions.{col, column}\n",
    "\n",
    "col(“someColumnName”)\n",
    "\n",
    "column(“someColumnName”)\n",
    "\n",
    "$”myColumn”\n",
    "\n",
    "#### Explicit Column References\n",
    "\n",
    "df.col(“count”)\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import org.apache.spark.sql.functions.{expr, col, column}\n",
    "df.select(\n",
    "    df.col(\"DEST_COUNTRY_NAME\"),\n",
    "    $\"ORIGIN_COUNTRY_NAME\",\n",
    "    'count).show(2)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania| null|\n|    United States|            Croatia| null|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{expr, col, column}\n"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "res30: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string]\n"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "df.select(\n",
    "expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania| null|        false|\n",
      "|    United States|            Croatia| null|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "\"*\", // all original columns\n",
    "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\"\n",
    ").show(2)"
   ]
  },
  {
   "source": [
    "### Expressions\n",
    "\n",
    "An expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a function that takes as input one or more column names, resolves them and then potentially applies more expressions to create a single value for each record in the dataset. \n",
    "\n",
    "Importantly, this “single value” can actually be a complex type like a Map type or Array type.\n",
    "\n",
    "%scala\n",
    "\n",
    "import org.apache.spark.sql.functions.{expr, col}\n",
    "\n",
    "In this simple instance, expr(“someCol”) is equivalent to col(“someCol”).\n",
    "\n",
    "expr(“someCol - 5”) is the same transformation as performing col(“someCol”) - 5 or even\n",
    "expr(“someCol”) - 5. That’s because Spark compiles these to a logical tree specifying the order of operations. \n",
    "\n",
    "%scala\n",
    "\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "expr(“(((someCol + 5) * 200) - 6) < otherCol”)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Creating Rows\n",
    "\n",
    "You can create rows by manually instantiating a Row object with the values that below in each column. It’s important to note that only DataFrames have schema. Rows themselves do not have schemas. This means if you create a Row manually, you must specify the values in the same order as the schema of the DataFrame they may be appended to.\n",
    "\n",
    "%scala\n",
    "\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val myRow = Row(“Hello”, null, 1, false)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "We can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "myManualSchema: org.apache.spark.sql.types.StructType = StructType(StructField(some,StringType,true), StructField(col,StringType,true), StructField(names,LongType,false))\n",
       "myRows: Seq[org.apache.spark.sql.Row] = List([Hello,null,1])\n",
       "myRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[141] at parallelize at <console>:46\n",
       "myDf: org.apache.spark.sql.DataFrame = [some: string, col: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructField, StructType,StringType, LongType}\n",
    "\n",
    "val myManualSchema = new StructType(Array(\n",
    "        new StructField(\"some\", StringType, true),\n",
    "        new StructField(\"col\", StringType, true),\n",
    "        new StructField(\"names\", LongType, false)\n",
    "    ))\n",
    "\n",
    "val myRows = Seq(Row(\"Hello\", null, 1L))\n",
    "val myRDD = spark.sparkContext.parallelize(myRows)\n",
    "val myDf = spark.createDataFrame(myRDD, myManualSchema)\n",
    "\n",
    "myDf.show()"
   ]
  },
  {
   "source": [
    "#### Converting to Spark Types (Literals)\n",
    "\n",
    "Sometimes we need to pass explicit values into Spark that aren’t a new column but are just a value. This might be a constant value or something we’ll need to compare to later on. The way we do this is through literals. This is basically a translation from a given programming language’s literal value to one that Spark understands. Literals are expressions and can be used in the same way. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|something|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania| null|        1|\n",
      "|    United States|            Croatia| null|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "df.select(\n",
    "        expr(\"*\"),\n",
    "        lit(1).as(\"something\")\n",
    "    ).show(2)"
   ]
  },
  {
   "source": [
    "#### Adding/Renaming Columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- COUNT: long (nullable = true)\n",
      " |-- This Long Column-Name: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|This Long Column-Name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania| null|              Romania|\n",
      "|    United States|            Croatia| null|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dfWithLongColName: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "var dfWithLongColName = df\n",
    ".withColumn(\"This Long Column-Name\",expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "\n",
    "dfWithLongColName.printSchema\n",
    "dfWithLongColName.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+-------------------+-----+----------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|somecolumn|\n+-----------------+-------------------+-----+----------+\n|    United States|            Romania| null|   Romania|\n|    United States|            Croatia| null|   Croatia|\n+-----------------+-------------------+-----+----------+\nonly showing top 2 rows\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dfWithLongColName: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "dfWithLongColName = dfWithLongColName.withColumnRenamed(\"This Long Column-Name\", \"somecolumn\")\n",
    "\n",
    "dfWithLongColName.show(2)"
   ]
  },
  {
   "source": [
    "#### Changing a Column’s Type (cast)\n",
    "\n",
    "%sql\n",
    "\n",
    "SELECT cast(count as int) FROM dfTable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- COUNT: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.withColumn(\"count\", col(\"count\").cast(\"int\")).printSchema()"
   ]
  },
  {
   "source": [
    "#### Filtering Rows\n",
    "\n",
    "There are two methods to perform this operation, we can use where or filter and they both will perform the same operation and accept the same argument types when used with DataFrames. \n",
    "\n",
    "The Dataset API has slightly different options.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "colCondition: Array[org.apache.spark.sql.Row] = Array()\n",
       "conditional: Array[org.apache.spark.sql.Row] = Array()\n"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "val colCondition = df.filter(col(\"count\") < 2).take(2)\n",
    "val conditional = df.where(\"count < 2\").take(2)"
   ]
  },
  {
   "source": [
    "#### Getting Unique Rows\n",
    "\n",
    "%sql\n",
    "\n",
    "SELECT COUNT(DISTINCT ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME) FROM dfTable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "res47: Long = 125\n"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count"
   ]
  },
  {
   "source": [
    "#### Sorting Rows\n",
    "\n",
    "When we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of a DataFrame. There are two equivalent operations to do this **sort** and **orderBy** that work the exact same way. They accept both column expressions and strings as well as multiple columns. The default is to sort in ascending order."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania| null|\n",
      "|    United States|            Croatia| null|\n",
      "|    United States|            Ireland| null|\n",
      "|            Egypt|      United States| null|\n",
      "|    United States|              India| null|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States| null|\n",
      "|           Angola|      United States| null|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+-------------------+-----+\n",
      "|  DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|COUNT|\n",
      "+-------------------+-------------------+-----+\n",
      "|            Algeria|      United States| null|\n",
      "|             Angola|      United States| null|\n",
      "|           Anguilla|      United States| null|\n",
      "|Antigua and Barbuda|      United States| null|\n",
      "|          Argentina|      United States| null|\n",
      "+-------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(desc(\"count\"), asc(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}